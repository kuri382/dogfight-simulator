<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.14.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>ASRCAISim1: HandyRL(の改変版)を用いた強化学習サンプル</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(function() { init_search(); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="asrc_doc_stylesheet.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">ASRCAISim1
   </div>
  </td>
    <td>        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <span id="MSearchSelect" class="search-icon" onmouseover="return searchBox.OnSearchSelectShow()" onmouseout="return searchBox.OnSearchSelectHide()"><span class="search-icon-dropdown"></span></span>
          <input type="text" id="MSearchField" value="" placeholder="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><div id="MSearchCloseImg" class="close-icon"></div></a>
          </span>
        </div>
</td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.14.0 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search/",'.html');
</script>
<script type="text/javascript">
$(function() { codefold.init(); });
</script>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(function(){initNavTree('a198026.html','',''); });
</script>
<div id="container">
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">HandyRL(の改変版)を用いた強化学習サンプル </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>sample/scripts/R7Contest/HandyRLSample以下には、 <a href="https://gihub.com/DeNA/HandyRL">HandyRL</a>を本シミュレータ用に改変したものを 用いて深層強化学習を行うサンプルを同梱している。</p>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_modification"></a>
元の HandyRL に対する改変・機能追加の概要</h1>
<p>本サンプルでは HandyRL に以下のような改変・機能追加を行っている。</p><ul>
<li>Discrete以外のaction_spaceに対応</li>
<li>MatchMakerによる行動判断モデルの動的選択に対応</li>
<li>Imitationに対応</li>
<li>Ape-X型のε-greedyに対応</li>
<li>SummaryWriterによるTensorboard形式のログ出力に対応</li>
<li>turn_basedな環境に対する特殊化を削除</li>
<li>evalモードの削除</li>
<li>ReplayBufferを分離し、優先度付き経験再生を使用可能に</li>
<li>rayによる並列化に変更</li>
<li>複数のLearnerを連接可能に</li>
<li>agent.pyにおいて温度パラメータによる焼きなましを無効化</li>
<li>学習率を固定化可能に</li>
<li>eval_rate(学習用と評価用のエピソードの割合)を固定化可能に</li>
</ul>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_run_train"></a>
学習の実行方法</h1>
<p>sample/scripts/R7Contest/HandyRLSample 上で、 </p><div class="fragment"><div class="line">python main.py R7_contest_open_sample_M.yaml --train</div>
</div><!-- fragment --><p> を実行するとオープン部門のシナリオで1体で陣営全体を操作する行動判断モデルについて、 <a class="el" href="a198028.html">R7ContestTwoTeamCombatMatchMaker</a> を用いたSelf-Play による学習が行われる。</p>
<p>学習結果は./results/Open/Multi/YYYYmmddHHMMSS 以下に保存される。 なお、"M"を"S" に変更すると、1 体で1機を操作する行動判断モデルとなる。 また、"open"を"youth"に変更するとユース部門のシナリオとなる。</p>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_run_evaluation"></a>
学習済モデルの評価</h1>
<p>学習済モデルは、./results/Open/Multi/YYYYmmddHHMMSS/policies/checkpoints 以下に保存されている。 このモデルの評価は、７項に示す sample/scripts/R7Contest/MinimumEvaluation 以下 に同梱されているスクリプトを用いて行うことができる。</p>
<p>同ディレクトリ中の candidates.json を開き、例えば "test":{ "userModuleID":"HandyRLSample01M", "args":{"weightPath":&lt;学習済の.pth ファイルのフルパス&gt;} } のように候補を追加し、同ディレクトリ上で </p><div class="fragment"><div class="line">python evaluator.py &quot;test&quot; &quot;Rule-Fixed&quot; -n 10 -v -l &quot;eval_test.csv&quot;</div>
</div><!-- fragment --><p> と実行すると、初期行動判断モデルとの対戦による学習済モデルの評価を行うことができる。</p>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_yaml_format"></a>
yaml の記述方法</h1>
<p>yamlの記述方法は以下のとおりである。 実際の値の例はサンプルファイルを参照されたい。</p>
<div class="fragment"><div class="line">save_dir: str # 保存先を記述する。</div>
<div class="line"> </div>
<div class="line">ray_args: # rayクラスタに関する設定を記述する。</div>
<div class="line">    namespace: str # rayクラスタの名前空間を指定する。</div>
<div class="line">    head_ip_address: # rayクラスタの本体のIPアドレスを指定する。&quot;auto&quot;とすると自動選択となる。</div>
<div class="line">    entrypoint_ip_address: # 学習スクリプトを実行する端末のIPアドレスを指定する。</div>
<div class="line">                           # ここを127.0.0.1として新規クラスタが生成された場合、別プロセスからのアクセスができなくなるらしい。</div>
<div class="line">                           # アクセス可能なクラスタを事前に立ち上げておくか、127.0.0.1以外のIPアドレスで指定する必要がある。この例は前者の想定。</div>
<div class="line"> </div>
<div class="line">env_args: # シミュレーション環境に関する設定を記述する。</div>
<div class="line">    env: ASRCAISim1 # HandyRL用の環境クラス。実装は./handyrl/envs/ASRCAISim1.pyにある。</div>
<div class="line">    env_config: # 上記のenvに渡す引数</div>
<div class="line">        config: # SimulationManager のコンストラクタに渡すconfig。</div>
<div class="line">        - ./configs/R7_contest_open_mission_config.json # 戦闘シナリオの基本構造を指定</div>
<div class="line">        - ./configs/R7_contest_open_asset_placeholder.json # 機体性能変化用のプレースホルダー</div>
<div class="line">        - ./configs/R7_contest_learning_config_S.json # AgentConfigDispatcherとRewardの指定</div>
<div class="line">        - ./configs/R7_contest_agent_ruler_reward_models.json # Agent、Ruler、Rewardのmodel configの指定</div>
<div class="line">        - Manager:</div>
<div class="line">            ViewerType: None</div>
<div class="line">            Callbacks: {}</div>
<div class="line">            Loggers: {}</div>
<div class="line"> </div>
<div class="line">policy_config: # 行動判断ポリシーに関する設定</div>
<div class="line">    Learner: # 使用するポリシー名をキーとするdictで与える。</div>
<div class="line">             # このポリシー名に対応するAgentConfigDispatcherのaliasが</div>
<div class="line">             # ./configs/R7_contest_learning_config_S.json (env_configで与えたファイル)</div>
<div class="line">             # に記述されている必要がある。</div>
<div class="line">        multi_port: bool # 中央集権型ならtrue、分散型ならfalse</div>
<div class="line">        active_limit: int # MatchMakerで選択候補とする過去の重みの下図</div>
<div class="line">        is_internal: bool # NNモデルが不要なポリシーかどうか(現在はこのフラグは不使用)</div>
<div class="line">        populate: # 重み保存に関する設定</div>
<div class="line">            first_population: int # 初回保存タイミング(エピソード数)</div>
<div class="line">            interval: int # 保存間隔(エピソード数)</div>
<div class="line">            on_start: bool # 学習開始時点の重みを保存するかどうか</div>
<div class="line">        rating_initial: # 初期イロレーティング</div>
<div class="line">        rating_fixed: # イロレーティングを固定するかどうか</div>
<div class="line">        initial_weight: str or null # 初期重みを読み込む場合はそのパスを指定する</div>
<div class="line">        model_class: # NNクラス名を指定する。./main.pyのcustom_classesに登録したもの指定する。</div>
<div class="line">        model_config: dict # NNクラスのコンストラクタに渡す引数を記述する。</div>
<div class="line">    Initial: # observation,actionの入出力が不要なルールベースモデルの場合は以下のように指定できる</div>
<div class="line">        multi_port: false</div>
<div class="line">        active_limit: null</div>
<div class="line">        is_internal: true</div>
<div class="line">        populate: null</div>
<div class="line">        rating_initial: 1500</div>
<div class="line">        rating_fixed: true</div>
<div class="line">        initial_weight: null</div>
<div class="line">        model_class: DummyInternalModel</div>
<div class="line">        model_config: {}</div>
<div class="line">    Expert: # 教師役のtrajectoryを模倣する場合はこれを用いる。</div>
<div class="line">        multi_port: false</div>
<div class="line">        active_limit: null</div>
<div class="line">        is_internal: true</div>
<div class="line">        populate: null</div>
<div class="line">        rating_initial: 1500</div>
<div class="line">        rating_fixed: false</div>
<div class="line">        initial_weight: null</div>
<div class="line">        model_class: DummyInternalModel</div>
<div class="line">        model_config: {}</div>
<div class="line"> </div>
<div class="line">train_args: #学習に関する設定</div>
<div class="line">    Learner: # Learnerの識別名をキーとしたdictで与える。</div>
<div class="line">        node_designation: str or null # Learnerを置くIPアドレスの指定があれば書く。ここで127.0.0.1を指定してはいけないらしい。nullで問題ない。</div>
<div class="line">        num_gpus: float # GPUの数。基本的には1</div>
<div class="line">        match_monitor_class: str # 使用するMatchMonitorクラス名を指定。./main.pyのcustom_classesに登録したもの指定する。</div>
<div class="line">        turn_based_training: false # ターンベースな環境を前提とした学習にするかどうか。falseのみ。</div>
<div class="line">        observation: false # 行動しないAgentによる観測を許すかどうか。原則false。</div>
<div class="line">        auto_tune_lr: bool # 本家HandyRLでは学習率はハードコーディングでスケジューリングされている。</div>
<div class="line">                            # その有効/無効を切り替えるフラグ</div>
<div class="line">        lr: float # 本家HandyRLのスケジューリングを無効化する場合の学習率</div>
<div class="line">        gamma: float # 報酬割引率</div>
<div class="line">        forward_steps: int # 学習データ一つあたりのステップ数</div>
<div class="line">        burn_in_steps: int  # burn-inのステップ数</div>
<div class="line">        compress_steps: int # エピソードデータの圧縮を行う単位(ステップ数)</div>
<div class="line">        entropy_regularization: # エントロピー正則化の強さ</div>
<div class="line">        entropy_regularization_decay: # エントロピー正則化の時間方向の減衰</div>
<div class="line">        separate_policy_gradients: bool # trueにすると、方策勾配のlossがaction要素ごとに独立に計算される。</div>
<div class="line">        exploration_config: # 探索に関する設定</div>
<div class="line">            use_exploration: bool # Ape-X型のε-greedyによる探索を有効にするかどうか。</div>
<div class="line">            only_one_explorer_per_policy: bool # 複数のpolicyが存在する場合に1ステップあたりに探索を行うpolicyを一つに限定するかどうか</div>
<div class="line">            eps_start: float # εの最大値の初期値</div>
<div class="line">            eps_end: float # εの最大値の終端値</div>
<div class="line">            eps_decay: float # εの減衰が完了するエポック数。負数を指定すると減衰なし。</div>
<div class="line">            alpha: float # εのworkerごとの分布を定めるパラメータ</div>
<div class="line">            cycle: int # εを定める際にworker数を水増しするパラメータ。</div>
<div class="line">        checkpoint_interval: int # checkpointの保存周期(epoch単位)</div>
<div class="line">        update_episodes: int # 重みの更新を行う周期(episode単位)</div>
<div class="line">        batch_size: int # バッチサイズ</div>
<div class="line">        minimum_episodes: # 重みの学習を始めるタイミング(episode単位)</div>
<div class="line">        epochs: int # 終了するエポック数。負数の場合は自動終了しない。</div>
<div class="line">        num_batchers: int # Batcherインスタンスの数</div>
<div class="line">        eval_rate: float # 学習でなく評価に用いるエピソードの割合。本家HandyRLでは勝手にupdate_episode^(-0.15)とのmaxを取っていたものを無効化している。</div>
<div class="line">        worker:</div>
<div class="line">            num_parallel: int # workerの数</div>
<div class="line">        lambda: float # TD(λ)のλ</div>
<div class="line">        policy_target: str # actor lossのターゲット。&#39;UPGO&#39; &#39;VTRACE&#39; &#39;TD&#39; &#39;MC&#39;から選択。</div>
<div class="line">        value_target: str # critic lossのターゲット。&#39;VTRACE&#39; &#39;TD&#39; &#39;MC&#39;から選択。</div>
<div class="line">        seed: int # 乱数のシード</div>
<div class="line">        policy_to_train: str # 学習対象のpolicy名。policy_configに挙げたものから選択する。</div>
<div class="line">        policy_to_imitate: [] # 摸倣学習を実施する際は教師役のpolicy名を与える。policy_configに挙げたものから選択する。</div>
<div class="line">        disable_rl_from_imitator: bool # 教師役のtrajectoryに対するオフポリシー強化学習を無効化するかどうか。</div>
<div class="line">        imitation_beta: float # 模倣時に報酬(Advantage)を参照してlossの重み付けをする場合の係数。</div>
<div class="line">        imitation_adv_ma_initial：float # 模倣時に報酬(Advantage)を参照してlossの重み付けをする場合にAdvantageを正規化するための移動平均の初期値。</div>
<div class="line">        imitation_adv_ma_update_rate: float # 模倣時に報酬(Advantage)を参照してlossの重み付けをする場合にAdvantageを正規化するための移動平均の更新量。</div>
<div class="line">        imitation_kl_threshold: float # 模倣loss計算時のKLダイバージェンスのクリッピングを行う値。</div>
<div class="line">        imitation_loss_scale: float # 模倣lossの係数</div>
<div class="line">        imitation_loss_threshold: float # 模倣loss計算時のlossのクリッピングを行う値。</div>
<div class="line">        deterministic: #分布からのサンプリングでなくgreedyに行動をサンプリングするエピソードの確率。</div>
<div class="line">            g: float # 学習用エピソードにおける割合</div>
<div class="line">            e: float # 評価用エピソードにおける割合</div>
<div class="line">        record_hidden_in: bool # NNの内部状態をリプレイバッファに保存するかどうか。メモリ消費量が激増するためFalseを推奨する。</div>
<div class="line">        prioritized_replay: bool # 優先度付き経験再生を使用するかどうか。</div>
<div class="line">        replay_buffer_config: # リプレイバッファに関する設定</div>
<div class="line">            node_designation: str or null # リプレイバッファを置くノードのIPアドレス。ここでも127.0.0.1を指定してはいけないらしい。nullで問題ない。</div>
<div class="line">            independent_replay_for_each_player: bool # 各player単位でリプレイバッファに記録するかどうか(Trueを推奨)</div>
<div class="line">            discard_untrainable_players: bool # 学習対象以外のpolicyの情報を捨てるかどうか(Trueを推奨)</div>
<div class="line">            ignore_noaction_timesteps: bool # 誰も行動していない時刻のデータを無視するかどうか(Falseの場合は空の時刻として残す)</div>
<div class="line">            # ↓本家のReplayBufferを使用する場合</div>
<div class="line">            maximum_episodes: 3000 #保存するエピソード数の上限</div>
<div class="line">            # ↓PrioritizedReplayBufferを使用する場合</div>
<div class="line">            calc_initial_priority: bool # バッファへの追加時に初期優先度を計算するかどうか。</div>
<div class="line">            priority_target: str # 優先度を計算する際のadvantageのターゲット。&#39;VTRACE&#39; &#39;TD&#39; &#39;MC&#39; &#39;SAME&#39;から選択。</div>
<div class="line">                                    # &#39;SAME&#39;はcritic lossのターゲットと同じものを使用する。</div>
<div class="line">            capacity: int # リプレイバッファの容量(ステップ数単位)</div>
<div class="line">            alpha: float # サンプリングのパラメータ</div>
<div class="line">            beta: float # サンプリングのパラメータ</div>
<div class="line">            beta_decay: int # サンプリングのパラメータ</div>
<div class="line">            store_interval: int # リプレイバッファのデータ点として登録するステップ数の間隔</div>
<div class="line">            eta: float # サンプリングのパラメータ</div>
<div class="line">            eps: float # サンプリングのパラメータ</div>
<div class="line">        worker_options:</div>
<div class="line">            num_cpus: float # 1 workerあたりのCPU。基本的に1でよい。</div>
<div class="line"> </div>
<div class="line">match_maker_args: # MatchMakerに関する設定</div>
<div class="line">    match_maker_class: str # 使用するMatchMakerクラス名を指定。./main.pyのcustom_classesに登録したもの指定する。</div>
<div class="line">    node_designation: str or null # MatchMakerを置くノードのIPアドレスを指定する。127.0.0.1を指定してはいけないらしい。nullで問題ない。</div>
<div class="line">    seed: # 乱数のシード</div>
<div class="line">    match_config: # MatchMakerのコンストラクタ引数を記述する。以下はR7ContestTwoTeamCombatMatchMakerの場合の例</div>
<div class="line">        #expert_ratio: float # 模倣学習を行う際のみ指定する。教師役による対戦の割合</div>
<div class="line">        warm_up_episodes: int # 学習初期に初期行動判断モデルのみと対戦させる場合、そのエピソード数</div>
<div class="line">        youth: bool # ユース部門の場合にTrueとする。</div>
<div class="line">        symmetric_initial_state: # 戦闘機の初期配置を点対称にするかどうか。</div>
<div class="line">        initial_state_number: 5 # 陣営の全機数。5で固定。</div>
<div class="line">        initial_state_lower: # Blue(teams[0])だった場合に設定する初期条件の下限値。省略時は置換しない。</div>
<div class="line">            pos: [float,float,float] # 初期位置</div>
<div class="line">            vel: float # 初期速度</div>
<div class="line">            heading: float # 初期方位(真北を0とし東回りを正とする)</div>
<div class="line">        initial_state_upper: # Blue(teams[0])だった場合に設定する初期条件の上限値。省略時は置換しない。</div>
<div class="line">            pos: [float,float,float] # 初期位置</div>
<div class="line">            vel: float # 初期速度</div>
<div class="line">            heading: float # 初期方位(真北を0とし東回りを正とする)</div>
<div class="line">        asset_spec_randomization: bool # 機体性能をランダム化する場合にtrueとする。(youth==Falseのときのみ有効)</div>
<div class="line">        symmetric_randomization: bool # 機体性能を両陣営で対象となるようにランダム化するかどうか。オープン部門の設定はTrue</div>
<div class="line">        heterogeneous_randomization: bool # 機体性能を陣営内でバラバラにランダム化するかどうか。オープン部門の設定はTrue</div>
<div class="line">        randomized_asset_spec: # 各機体性能の変動範囲</div>
<div class="line">            rcs_scale: [float,float] # RCSスケール</div>
<div class="line">            radar_range: [float,float] # レーダ探知距離(基準値からの倍率で指定)</div>
<div class="line">            radar_coverage: [float,float] # レーダ探知覆域(正面からの角度で指定)</div>
<div class="line">            maximum_speed: [float,float] # 機体最大速度(基準値からの倍率で指定)</div>
<div class="line">            num_missiles: [int,int] # 初期誘導弾数</div>
<div class="line">            missile_thrust: [float,float] # 誘導弾推力(基準値からの倍率で指定)</div>
<div class="line">            shot_approval_delay: [float,float] # 射撃承認までの遅延時間(秒で指定)</div>
</div><!-- fragment --><h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_nn"></a>
yaml で定義可能なニューラルネットワークのサンプル</h1>
<p>また、このobservationとactionのspaceに対応したニューラルネットワークのサンプルを、</p><ul>
<li><a class="elRef" href="../R7ContestSample/a191148.html">R7ContestTorchNNSampleForHandyRL</a> に実装しているのでこちらも参考にされたい。</li>
</ul>
<p>ニューラルネットワークの構造を yaml 上で定義できる HandyRL 用 NN モデルの簡易的なサンプルクラスを、 <a class="elRef" href="../R7ContestSample/index.html#mainpage_R7ContestSample">R7ContestSample</a>プラグインに <a class="elRef" href="../R7ContestSample/a191148.html">R7ContestTorchNNSampleForHandyRL</a> として実装している。</p>
<p>また、そのためのユーティリティとして <a class="elRef" href="../R7ContestSample/a191143.html">GenericTorchModelUtil.py</a>に、 与えられた dict から動的に torch.nn.Module を生成する GenericLayers クラスを実装している。 GenericLayers は、torch.nn のクラス名とコンストラクタ引数の組を並べることで、 torch.nn.Sequential に類似した形でニューラルネットワークを生成するものとなっている。</p>
<p><a class="elRef" href="../R7ContestSample/a191148.html">R7ContestTorchNNSampleForHandyRL</a> は、 この GenericLayers を部分モデルとして使用し、HandyRL で使用可能なニューラルネットワークモデルとして組み立てたものである。</p>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_custom_classes"></a>
カスタムクラスの使用</h1>
<p>本サンプルは、ニューラルネットワーク(本シミュレータにおける Policy に相当。HandyRL における呼称では"model")、 <a class="elRef" href="../MatchMaker/a189216.html">MatchMaker</a> 、<a class="elRef" href="../HandyRLUtility/a188953.html">行動確率分布</a>についてカスタムクラスを使用できるような機構を 有している。sample/scripts/R7Contest/HandyRLSample/main.py において、 </p><div class="fragment"><div class="line">custom_classes={</div>
<div class="line">    <span class="comment"># models</span></div>
<div class="line">    <span class="stringliteral">&quot;R7ContestTorchNNSampleForHandyRL&quot;</span>: R7ContestTorchNNSampleForHandyRL,</div>
<div class="line">    <span class="stringliteral">&quot;DummyInternalModel&quot;</span>: DummyInternalModel,</div>
<div class="line">    <span class="comment"># match maker</span></div>
<div class="line">    <span class="stringliteral">&quot;R7ContestTwoTeamCombatMatchMaker&quot;</span>: R7ContestTwoTeamCombatMatchMaker,</div>
<div class="line">    <span class="stringliteral">&quot;TwoTeamCombatMatchMonitor&quot;</span>: TwoTeamCombatMatchMonitor,</div>
<div class="line">    <span class="comment"># action distribution class getter</span></div>
<div class="line">    <span class="stringliteral">&quot;actionDistributionClassGetter&quot;</span>: getActionDistributionClass,</div>
<div class="line">}</div>
</div><!-- fragment --><p> のように、識別名とクラスオブジェクトを対応付けた dict を定義しておき、 前述の yaml で指定することによって、これらの中から選択して使用できるようになっている。</p>
<p>ニューラルネットワークのクラスは policy_config において Policy ごとに"model_class"キーで指定する。</p>
<p><a class="elRef" href="../MatchMaker/a189216.html">MatchMaker</a> のクラスは match_maker_args において"match_maker_class"キーで指定し、 MatchMonitor のクラスは train_args において"match_monitor_class"キーで指定する。</p>
<p>行動確率分布は policy_config において Policy ごとに"model_config"キーとして与える dict 内で "actionDistributionClassGetter"キーで指定する(省略可。)</p>
<h1 class="doxsection"><a class="anchor" id="section_r7_contest_handyrl_sample_logging"></a>
学習ログの構成</h1>
<p>本サンプルで保存されるログは以下のような構成となる。 なお、拡張子 pth で保存される Policy の重みは torch.nn.Module.state_dict()の返り値を torch.save(path)で保存したものであり、 torch.load(path)で読み込んで torch.load_state_dict に与えることで復元可能なものである。 </p><div class="fragment"><div class="line">&lt;save_dir&gt; #yaml で指定した save_dir</div>
<div class="line">└YYYYmmddHHMMSS #学習開始時のタイムスタンプで作成</div>
<div class="line">　├policies #Policy の重みに関するログ</div>
<div class="line">　│├checkpoints #学習対象 Policy のエポックごとの重み</div>
<div class="line">　││├&lt;policy&gt;-&lt;epoch&gt;.pth</div>
<div class="line">　││└&lt;policy&gt;-latest.pth #最新の重み</div>
<div class="line">　│└initial_weights #各 Policy の初期重み</div>
<div class="line">　│　└&lt;policy&gt;.pth</div>
<div class="line">　├matches #MatchMaker に関するログ</div>
<div class="line">　│├checkpoints #チェックポイント</div>
<div class="line">　││└MatchMaker-&lt;epoch&gt;.dat</div>
<div class="line">　│└matches_YYYYmmddHHMMSS.csv #MatchMaker が生成した全対戦結果のログ</div>
<div class="line">　└logs #SummaryWriter による Tensorboard ログ</div>
<div class="line">　　└&lt;Learner&gt; # train_argsで指定した各Learner名のディレクトリ</div>
<div class="line">　　　└events.out.tfevents.～ #Tensorboard ログ</div>
</div><!-- fragment --> </div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<div id="page-nav" class="page-nav-panel">
<div id="page-nav-resize-handle"></div>
<div id="page-nav-tree">
<div id="page-nav-contents">
</div><!-- page-nav-contents -->
</div><!-- page-nav-tree -->
</div><!-- page-nav -->
</div><!-- container -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.14.0 </li>
  </ul>
</div>
</body>
</html>
