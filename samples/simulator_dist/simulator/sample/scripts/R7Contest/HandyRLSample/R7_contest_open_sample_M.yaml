
save_dir: ./results/Open/Multi

ray_args:
    namespace: ASRCAISim1
    head_ip_address: auto
    entrypoint_ip_address: 127.0.0.1 #ここを127.0.0.1として新規クラスタが生成された場合、別プロセスからのアクセスができなくなるらしい。
                                     #アクセス可能なクラスタを事前に立ち上げておくか、127.0.0.1以外のIPアドレスで指定する必要がある。この例は前者の想定。

env_args:
    env: ASRCAISim1
    env_config:
        config: 
        - ./configs/R7_contest_open_mission_config.json
        - ./configs/R7_contest_open_asset_placeholder.json
        - ./configs/R7_contest_learning_config_M.json
        - ./configs/R7_contest_agent_ruler_reward_models.json
        - Manager:
            ViewerType: None
            Callbacks: {}
            Loggers: {}

policy_config:
    Learner:
        multi_port: true
        active_limit: 50
        is_internal: false
        populate:
            first_population: 1000
            interval: 1000
            on_start: false
        rating_initial: 1500
        rating_fixed: false
        initial_weight: null # /designate/your/weight/path/hoge.pth
        model_class: R7ContestTorchNNSampleForHandyRL
        model_config:
            actionDistributionClassGetter: actionDistributionClassGetter
            use_lstm: true
            lstm_cell_size: 256
            lstm_num_layers: 1
            lstm_dropout: 0.2
            common:
                layers:
                    - ["Linear",{"out_features": 16}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 16}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 16}],
                            ["BatchNorm1d",{}]
                        ]}]
            parent:
                layers:
                    - ["Linear",{"out_features": 64}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
            friend:
                layers:
                    - ["Linear",{"out_features": 64}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
            enemy:
                layers:
                    - ["Linear",{"out_features": 64}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
            friend_missile:
                layers:
                    - ["Linear",{"out_features": 64}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
            enemy_missile:
                layers:
                    - ["Linear",{"out_features": 64}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
                    - ["ReLU",{}]
                    - ["ResidualBlock",{
                        "layers":[
                            ["Linear",{"out_features": 64}],
                            ["BatchNorm1d",{}]
                        ]}]
            entity_equivariant:
                type: SetTransformer
                layers:
                    - ["ISAB",{"num_heads": 4,"num_inds": 16}]
                    - ["ISAB",{"num_heads": 4,"num_inds": 16}]
                #type: TransformerEncoder
                #layers:
                #    - ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                #    - ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
            entity_invariant:
                type: SetTransformer
                layers:
                    - ["ISAB",{"num_heads": 4,"num_inds": 16}]
                    - ["PMA",{"num_heads": 4,"num_seeds": 1}]
                #type: TransformerDecoder
                #layers:
                #    - ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                #    - ["TransformerDecoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
            merge:
                layers:
                    - ["ReLU",{}]
                    - ["Linear",{"out_features": 256}]
                    - ["ReLU",{}]
                    - ["Linear",{"out_features": 256}]
                    - ["ReLU",{}]
            apply_merged_to_entity:
                #type: SetTransformerWithQuery
                #layers:
                #    - ["ISAB",{"num_heads": 4,"num_inds": 16}]
                #    - ["ISAB",{"num_heads": 4,"num_inds": 16}]
                #    - ["PMAWithQuery",{"num_heads": 4}]
                type: TransformerDecoder
                layers:
                    - ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                    - ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                    - ["TransformerDecoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                    - ["TransformerDecoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
            critic:
                value: # 終了時の勝敗(±1)を割引率1で用いる本家HandyRL固有のcriticブランチ(NNにブランチを作らなければ無視される)
                    layers:
                        - ["Linear",{"out_features": 64}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 64}]
                        - ["ReLU",{}]
                return: # 環境側の本来の即時報酬を、yamlで指定した割引率で用いる通常のcriticブランチ(NNにブランチを作らなければ無視される)
                    layers:
                        - ["Linear",{"out_features": 64}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 64}]
                        - ["ReLU",{}]
            actor:
                turn:
                    apply_mask: True
                    layers:
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                pitch:
                    apply_mask: True
                    layers:
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                accel:
                    apply_mask: True
                    layers:
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                target:
                    apply_mask: True
                    type: TransformerDecoder
                    layers:
                        #- ["TransformerEncoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                        - ["TransformerDecoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                        - ["TransformerDecoderLayer",{"nhead": 4,"dim_feedforward": 16,"dropout": 0.1,"batch_first": true}]
                shotInterval:
                    apply_mask: True
                    layers:
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                shotThreshold:
                    apply_mask: True
                    layers:
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
                        - ["Linear",{"out_features": 32}]
                        - ["ReLU",{}]
    Initial:
        multi_port: false
        active_limit: null
        is_internal: true
        populate: null
        rating_initial: 1500
        rating_fixed: true
        initial_weight: null
        model_class: DummyInternalModel
        model_config: {}
# 模倣学習(Behavior cloning又はOffline RL)を実施する場合はコメントアウトを外す
# R5_contest_learning_config_M.jsonにおいて、所望のExpertWrapperのalias名と一致していることを確認すること
#    Expert:
#        multi_port: true
#        active_limit: null
#        is_internal: true
#        populate: null
#        rating_initial: 1500
#        rating_fixed: false
#        initial_weight: null
#        model_class: DummyInternalModel
#        model_config: {}

train_args:
    Learner:
        node_designation: null #127.0.0.1を指定してはいけない。
        num_gpus: 1
        match_monitor_class: TwoTeamCombatMatchMonitor
        turn_based_training: false
        observation: false
        auto_tune_lr: false #本家HandyRLでは学習率はハードコーディングでスケジューリングされている。その有効/無効を切り替えるフラグ
        lr: 1.0e-4 #本家HandyRLのスケジューリングを無効化する場合の学習率
        gamma: 0.997
        forward_steps: 64
        burn_in_steps: 16  # for RNNs
        compress_steps: 16
        entropy_regularization: 4.0e-3
        entropy_regularization_decay: 0.1
        separate_policy_gradients: false
        exploration_config:
            use_exploration: true
            only_one_explorer_per_policy: false
            eps_start: 0.4
            eps_end: 0.4
            eps_decay: -1
            alpha: 7.0
            cycle: 16
        checkpoint_interval: 10
        update_episodes: 10
        batch_size: 128
        minimum_episodes: 10
        epochs: -1
        num_batchers: 4
        eval_rate: 0.1 #本家HandyRLでは勝手にupdate_episode^(-0.15)とのmaxを取っていたものを無効化している。
        worker:
            num_parallel: 12
        lambda: 0.7
        policy_target: UPGO # 'UPGO' 'VTRACE' 'TD' 'MC'
        value_target: TD # 'VTRACE' 'TD' 'MC'
        seed: 0
        policy_to_train: Learner
        policy_to_imitate: [] # ["Expert"] #摸倣学習を実施する際は教師役のpolicy名を与える
        imitation_beta: 0.6
        imitation_kl_threshold: 1.0
        imitation_loss_scale: 1.0
        imitation_loss_threshold: 1.0
        deterministic:
            g: 0.1
            e: 0.1
        record_hidden_in: False # NNの内部状態をリプレイバッファに保存するか否か。メモリ消費量が激増するためFalseを推奨する。
        prioritized_replay: True
        replay_buffer_config:
            node_designation: null
            independent_replay_for_each_player: True # 各player単位でリプレイバッファに記録するかどうか
            discard_untrainable_players: True # 学習対象以外のpolicyの情報を捨てるかどうか
            ignore_noaction_timesteps: True # 誰も行動していない時刻のデータを無視するかどうか(Falseの場合は空の時刻として残す)
            # ↓本家のReplayBuffer
            maximum_episodes: 3000
            # ↓PrioritizedReplayBuffer
            calc_initial_priority: False
            priority_target: 'SAME' # 'VTRACE' 'TD' 'MC' 'SAME'
            capacity: 2000000
            alpha: 0.6
            beta: 0.4
            beta_decay: 6000000
            store_interval: 1
            eta: 0.9
            eps: 1.0e-8
        worker_options:
            num_cpus: 1.0


match_maker_args:
    match_maker_class: R7ContestTwoTeamCombatMatchMaker
    node_designation: null #127.0.0.1を指定してはいけない。
    seed: 12345
    match_config:
        #expert_ratio: 0.2 #模倣学習を行う際のみ指定する。
        warm_up_episodes: 0
        youth: false
        symmetric_initial_state: true
        initial_state_number: 5
        initial_state_lower:
            pos: [-20000.0,95000.0,-12000.]
            vel: 260.0
            heading: 225.0
        initial_state_upper:
            pos: [20000.0,105000.0,-6000.0]
            vel: 280.0
            heading: 315.0
        asset_spec_randomization: true
        symmetric_randomization: true
        heterogeneous_randomization: true
        randomized_asset_spec:
            rcs_scale: [0.4,1.0] # 探知距離にして約0.8〜1.0倍
            radar_range: [0.8,1.2]
            radar_coverage: [60.0,120.0]
            maximum_speed: [0.8,1.2]
            num_missiles: [4,8]
            missile_thrust: [0.8,1.2]
            shot_approval_delay: [1,5]
